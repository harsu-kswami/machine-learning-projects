{
    "metadata": {
      "version": "1.0",
      "created": "2025-10-03",
      "description": "Sample vocabulary for positional encoding experiments",
      "tokenizer_type": "simple",
      "vocab_size": 1000,
      "special_tokens": {
        "pad_token": "<PAD>",
        "unk_token": "<UNK>",
        "bos_token": "<BOS>",
        "eos_token": "<EOS>",
        "mask_token": "<MASK>"
      }
    },
    "vocabulary": {
      "<PAD>": 0,
      "<UNK>": 1,
      "<BOS>": 2,
      "<EOS>": 3,
      "<MASK>": 4,
      "the": 5,
      "of": 6,
      "and": 7,
      "to": 8,
      "a": 9,
      "in": 10,
      "is": 11,
      "it": 12,
      "you": 13,
      "that": 14,
      "he": 15,
      "was": 16,
      "for": 17,
      "on": 18,
      "are": 19,
      "as": 20,
      "with": 21,
      "his": 22,
      "they": 23,
      "i": 24,
      "at": 25,
      "be": 26,
      "this": 27,
      "have": 28,
      "from": 29,
      "or": 30,
      "one": 31,
      "had": 32,
      "by": 33,
      "word": 34,
      "but": 35,
      "not": 36,
      "what": 37,
      "all": 38,
      "were": 39,
      "we": 40,
      "when": 41,
      "your": 42,
      "can": 43,
      "said": 44,
      "there": 45,
      "each": 46,
      "which": 47,
      "she": 48,
      "do": 49,
      "how": 50,
      "their": 51,
      "if": 52,
      "will": 53,
      "up": 54,
      "other": 55,
      "about": 56,
      "out": 57,
      "many": 58,
      "then": 59,
      "them": 60,
      "these": 61,
      "so": 62,
      "some": 63,
      "her": 64,
      "would": 65,
      "make": 66,
      "like": 67,
      "into": 68,
      "him": 69,
      "has": 70,
      "two": 71,
      "more": 72,
      "very": 73,
      "what": 74,
      "know": 75,
      "just": 76,
      "first": 77,
      "get": 78,
      "over": 79,
      "think": 80,
      "also": 81,
      "back": 82,
      "after": 83,
      "use": 84,
      "man": 85,
      "new": 86,
      "now": 87,
      "old": 88,
      "see": 89,
      "way": 90,
      "who": 91,
      "boy": 92,
      "did": 93,
      "its": 94,
      "let": 95,
      "put": 96,
      "say": 97,
      "end": 98,
      "why": 99,
      "try": 100,
      "model": 101,
      "models": 102,
      "training": 103,
      "data": 104,
      "learning": 105,
      "machine": 106,
      "neural": 107,
      "network": 108,
      "networks": 109,
      "deep": 110,
      "language": 111,
      "text": 112,
      "attention": 113,
      "transformer": 114,
      "transformers": 115,
      "encoding": 116,
      "encodings": 117,
      "position": 118,
      "positional": 119,
      "sequence": 120,
      "sequences": 121,
      "token": 122,
      "tokens": 123,
      "layer": 124,
      "layers": 125,
      "head": 126,
      "heads": 127,
      "multi": 128,
      "self": 129,
      "dimension": 130,
      "dimensions": 131,
      "embedding": 132,
      "embeddings": 133,
      "matrix": 134,
      "vector": 135,
      "vectors": 136,
      "weight": 137,
      "weights": 138,
      "parameter": 139,
      "parameters": 140,
      "gradient": 141,
      "gradients": 142,
      "backpropagation": 143,
      "optimization": 144,
      "optimizer": 145,
      "loss": 146,
      "function": 147,
      "activation": 148,
      "softmax": 149,
      "relu": 150,
      "gelu": 151,
      "normalization": 152,
      "dropout": 153,
      "regularization": 154,
      "overfitting": 155,
      "underfitting": 156,
      "generalization": 157,
      "validation": 158,
      "test": 159,
      "evaluation": 160,
      "metric": 161,
      "metrics": 162,
      "accuracy": 163,
      "precision": 164,
      "recall": 165,
      "f1": 166,
      "score": 167,
      "performance": 168,
      "bert": 169,
      "gpt": 170,
      "t5": 171,
      "encoder": 172,
      "decoder": 173,
      "architecture": 174,
      "feed": 175,
      "forward": 176,
      "residual": 177,
      "connection": 178,
      "skip": 179,
      "batch": 180,
      "size": 181,
      "length": 182,
      "vocabulary": 183,
      "vocab": 184,
      "tokenization": 185,
      "tokenizer": 186,
      "bpe": 187,
      "wordpiece": 188,
      "sentencepiece": 189,
      "subword": 190,
      "character": 191,
      "word": 192,
      "sentence": 193,
      "document": 194,
      "corpus": 195,
      "dataset": 196,
      "datasets": 197,
      "preprocessing": 198,
      "processing": 199,
      "nlp": 200
    },
    "reverse_vocabulary": {
      "0": "<PAD>",
      "1": "<UNK>",
      "2": "<BOS>",
      "3": "<EOS>",
      "4": "<MASK>",
      "5": "the",
      "6": "of",
      "7": "and",
      "8": "to",
      "9": "a",
      "10": "in",
      "11": "is",
      "12": "it",
      "13": "you",
      "14": "that",
      "15": "he",
      "16": "was",
      "17": "for",
      "18": "on",
      "19": "are",
      "20": "as",
      "21": "with",
      "22": "his",
      "23": "they",
      "24": "i",
      "25": "at",
      "26": "be",
      "27": "this",
      "28": "have",
      "29": "from",
      "30": "or",
      "31": "one",
      "32": "had",
      "33": "by",
      "34": "word",
      "35": "but",
      "36": "not",
      "37": "what",
      "38": "all",
      "39": "were",
      "40": "we"
    },
    "statistics": {
      "total_tokens": 200,
      "special_tokens": 5,
      "regular_tokens": 195,
      "average_token_length": 4.2,
      "max_token_length": 15,
      "min_token_length": 1
    }
  }
  